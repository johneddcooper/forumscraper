import BeautifulSoup
from BeautifulSoup import BeautifulSoup as bs
from math import sqrt
from itertools import chain
import sys
import re
import argparse
import pdb

"""
Note on data structures.
What do the functions accept as arguments?

soup: BeautifulSoup.Soup object
    E.g. soup = bs(src)

L: list of BeautifulSoup.Tag objects. This is a one dimensional array
    E.g. [<html>...</html>, <head>...</head>, <body>...</body>, etc.]
    L = soup()

tag: BeautifulSoup.Tag object

na (name-attribute): a string generated by a call to name_attr(tag).
    It is a concatenation of a tag's name and attribute contents,
    with the special property that all strings of numbers are replaced
    by a single x. This is to match elements such as different posts,
    across their different IDs.

    For example, <div name="post_3421" border=1px> becomes "div-post_x-xpx"
    and has the same name-attr as any other div post with a border.

    The names of the attributes ("name", "border") are dropped.

D: Dictionary of {name-attributes: counts} within a given L. Thus all of the
    <div name="post_x"> (where x is a number) tags become clustered
    and counted as one. This is used in internal functions to determine which
    tags are systematically generated, as they would tend to appear
    extremely frequently compared to other tags at the same depth level across
    pages.

tbd: Tags by depth. A list of lists of tags, where the index is the corresponding depth.
    tbd[0] is the highest tag, tbd[1] contain its immediate children, and so forth.
    Returned by a call to get_tags_by_depth(soup)

dbfs: Depth by files. A list of tbds. dbfs[page][depth][tag]

fbds: Files by depth. zip(*dbfs). fbds[depth][page][tag]. This allows for easy
    access to a large number of tags per given depth.

page: The final data structure that contains all the useful extracted tags per
    page. It is still in progress, but in theory, the index should correspond
    to page[post][element], where element corresponds to the exact same element
    across posts (so all page[_][0] might contain "Post Date", or something).

    However, as it stands, there is an extra useless dimension to this array,
    so it looks like page[post][element_chunk][element], and element_chunk
    is always just length 1 and only contains element. That's a bug.

"""

##################################
#     public Scraper class       #
##################################

class Scraper:
    #############################
    #       Public methods      #
    #############################
    
    def __init__(self, soups=None):
        if soups:
            self.train(soups)

    def train(self, soups):
        """Train over list of soups to find location/names of relevant tags"""
       
        dbfs = map(get_tags_by_depth, soups)
        # dbfs = Depth by files [f1:[d1, d2, ...], f2:[d1, d2, ...],...]
        # f_i and f_j can be different lengths, depending on their max depth
        # len(dbfs) is always the number of pages

        fbds = zip(*dbfs)
        # fbds = Files by depth [d1:[f1, f2, ...], d2[f1, f2, ...], ...]
        # all d_i are the same length, because they all contain equal # files
        # each f_i entry is actually a BeautifulSoup tag, so fbds[0][0] might contain
        # "[<HTML>...</HTML>]", for example.
        # len(dbfs) is the maximum tag depth of all the pages.

        if len(fbds) < 3:
            print "Min depth not achieved."
            sys.exit()
        print "Beginning comparison at depth level", len(fbds)/3

        for i in range(len(fbds)/3, len(fbds)-1):
            if not all(same_tags(x1, x2) for x1 in fbds[i] for x2 in fbds[i]):
                print "Not all tags at depth level %d are the same"%i
                break

        t = [outliars(dbfs[x], i) for x in xrange(P)]
        map(strip_textless_tags, t)
        packed = map(extract_contents, t)
        contents, allnames, page = zip(*packed)
        pdb.set_trace()        

    def extract(self, soup):
        """Extracts content of a soup, given training data"""
        
        if not self.train_data:
            print "Error: Need to train data first."
            return

##################################
# Miscellanious numeric functions#
##################################

def mean(li):
    """returns mean of a list"""

    return float(sum(li))/len(li)

def std(li):
    """returns standard deviation of a list"""

    m = mean(li)
    return sqrt(mean([abs(x-m) for x in li]))


##################################
#       Internal functions       #
##################################

def get_tags_by_depth(soup):
    """get_tags_by_depth(soup)
    Takes a BeautifulSoup object. Returns a list [d0, d1, ..., dn],
    where d_i is a list of the BeautifulSoup.Tag objects at that given depth level.
    For example:
    d0=[<head>..</head>, <body>..</body>]
    d1=[<meta>.., <table>.., <script>..]

    """
    c = soup(recursive=False)
    if not c:
        return []
    tags_by_depth = [[t for t in c]]
    d = 1
    while True:
        tags = []
        for t in tags_by_depth[d-1]:
            tags.extend(t.findChildren(recursive=False))
    
        tags = filter(None, tags)
        if tags:
            tags_by_depth.append(tags)
        else:
            break

        d+=1
    return tags_by_depth

def same_tags(L1, L2):
    """same_tags(L1, L2)
    L1 and L2 are lists of BeautifulSoup.Tag objects
    Returns True iff all tags in one list are in the other.
    
    """ 
    
    s1 = set(tag_distribution(L1).keys())
    s2 = set(tag_distribution(L2).keys())


    return all(k1 in s2 for k1 in s1) and \
            all(k2 in s1 for k2 in s2)


def name_attr(tag, strip_nums=1, strip_orphans=1, strip_links=1):
    """name_attr(tag, strip_nums=1, strip_orphans=1, strip_links=1)
    Takes a BeautifulSoup.Tag object.
    Returns a string formatted "name-first_attribute-second_attribute".
    E.g. [<div class="post" style="font-weight:normal"></div>] returns "div-post-font-weight:normal"
    
    strip_nums: strips the numbers from the attribute, replacing any series of them with a single "x"
    E.g. [<div class=post232></div>] returns "div-postx"

    strip_orphans: returns an empty string for any tag that doesn't have attributes.
    
    strip_links: returns an empty string for any link tags.
    
    """
    if not tag:
        return ""
    if not strip_links or tag.name!= 'a':
        if tag.attrs:
            at = [tag.attrs[i][1] for i in xrange(len(tag.attrs))]
            at = "-".join(at)
            if strip_nums:
                at = re.sub(r"\d+","x", at)
            return "%s-%s"%(tag.name, at)
        elif not strip_orphans:
            return tag.name
    return ""

def get_tags_by_string(na, L):
    """get_tags_by_string(string, list of BeautifulSoup.Tag objects)
    Takes a name-attr string (as returned by name_attr()) and returns tags that match

    """
    
    return filter(lambda x: name_attr(x)==na, L)

def tag_distribution(L):
    """tag_distribution(L):
    L is a list of BeautifulSoup.Tag objects
    -Uses tag name and attributes to generate a dictionary of counts.
    -Strips all links.
    -Replaces all series of numbers with a single x.

    E.g. [BeautifulSoup("<div class=post32124></div>")] returns {'div-postx': 1}

    """
    ud = {}
    for x in L:
        n = name_attr(x)
        if n:
            if n in ud.keys():
                ud[n] += 1
            else:
                ud[n] = 1
    return ud


def get_outliars_from_dict(D):
    """get_outliars_from_dict(tag_dictionary)
    Internal function for outliars()
    Takes a dictionary where the values are integer counts.
    Returns a list of the keys where the values are two standard deviations above the mean.
    """
    m = mean(D.values())
    s = std(D.values())
    k = filter(lambda x: x[1] > m + 2*s, D.items())
    if k:
        return zip(*k)[0]

def outliars(tbd, d):
    """outliars(tags_by_depth, depth)
    Returns all tags that appear disproportionately often at the given depth.

    """

    outliars = get_outliars_from_dict(tag_distribution(tbd[d]))
    if not outliars:
        return []
    out_tags = (get_tags_by_string(x, tbd[d]) for x in outliars)
   
    out_tags_no_boring = []
    for i, x in enumerate(out_tags):
        if any(not e.text for e in x):
            continue
        if all(e.text==x[0].text for e in x):
            continue
        out_tags_no_boring.append(x)

    return out_tags_no_boring

def strip_textless_tags(page):
    """strip_textless_tags(page)
    Takes a nested list of page->outliar->tag
    Strips all the tags without any text that aren't images or don't contain images
    
    """
    
    i = 0
    c = 0
    for ol in page:
        for tag in ol:
            for comment in tag.findChildren(text=lambda txt:
                                isinstance(txt, BeautifulSoup.Comment)
            ):
                comment.extract()
                c+=1
            for child in tag.findChildren():
                if len(child.text) == 0 and not child.first('img')\
                                        and child.name != 'img':
                    child.extract()
                    i+=1

def extract_contents(page):
    """extract_contents(page)
    this takes each post, which consists of several large and nested
    div/tables, and extracts the subcontents that are identical across posts
    
    """

    newt = []
    allnames = [] 
    for item in page:
            
        children = [tag() for tag in item]
        children = filter(None, children)
        

        names = [filter(None, map(name_attr, ch)) for ch in children]
        names = filter(None, names) 

        # if an attribute is in all of the children, then extract it
        # and display it as an independent element
        
        if names:

            # which tags do all the posts have in common?
            # we want common tags because they are likely to indicate
            # structural similarities, like postdates/usernames
            # rather than tags from within posts
            
            common = reduce(lambda x, y: x.intersection(y), map(set, names))
            
            allnames.append(common) 
            # extract the actual tags based off their names/attributes
        
            newti = [[get_tags_by_string(com, c) for com in common] for c in children]
            
            # extract these tags from their parents
           
            [[[x.extract() for x in a] for a in b] for b in newti]

            # some of these tags are redundant. if they are exactly
            # the same across the posts, then strip them.
             
            newtz = zip(*newti)
            for i in range(len(newtz)):
                if all(x==newtz[i][0] for x in newtz[i]):
                    map(lambda n: n.pop(i), newti)
            if newti: 
                newt.append(newti)
    newtz = zip(*newti)
    allnames = reduce(lambda x, y: x.intersection(y), map(set, allnames)) 
    return newtz, allnames, page

def pn(L):
    """Print name: Calls name_attr() on all elements in list of BeautifulSoup.Tag objects."""
    return filter(None, map(name_attr, L))

def of(name, num, suffix=""):
    """open file: for internal testing purposes"""
    soups = []
    for i in xrange(1, int(num)+1):
        soups.append(bs(open("%s%s%s"%(name,str(i),suffix)).read()))
    return soups


if len(sys.argv) < 4:
    print "usage: python tester.py dir # suffix"

soups = of(sys.argv[1], sys.argv[2], sys.argv[3])
P = len(soups)

S = Scraper(soups)
